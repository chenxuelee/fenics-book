\fenicschapter{Discrete optimization of finite element matrix evaluation}
              {Discrete optimization of finite element matrix evaluation}
              {Robert C. Kirby, Matthew Gregg Knepley, Anders Logg, L. Ridgway Scott and Andy R. Terrel}
              {kirby-4}

The tensor contraction structure for the computation of the element
tensor~$A_T$ obtained in Chapter~\ref{chap:kirby-8}, enables not only the
construction of a compiler for variational forms, but an \emph{optimizing}
compiler. For typical variational forms, the reference tensor~$A^0$ has
significant structure that allows the element tensor~$A_T$ to be computed
on an arbitrary cell~$T$ at a lower computational cost. Reducing the
number of operations by making use of this structure, leads naturally
to several problems in discrete mathematics. This chapter introduces
some of the optimizations that are possible, and discusses compile-time
combinatorial optimization problems that form the core of the FErari
project \citep{KirbyLoggScottEtAl2006,KirbyScott2007,KirbyLogg2008},
which is the subject of Chapter~\ref{chap:kirby-3}.

We consider two basic kinds of optimizations in this chapter. First,
we consider relations between pairs of rows in the reference tensor.
This naturally leads to a graph that models proximity among these pairs.
If two rows are ``close'' together, then one may reuse results computed
with the first row to compute a desired quantity with the second.
The proximity of two such rows is computed using a Hamming distance
and linearity relations. This approach gives rise to a weighted graph
that is (almost) a metric space, so we designate such optimizations
as ``topological''.  Second, we consider relations between more than
two rows of the reference tensor. Such relations typically rely on
sets of rows, considered as vectors in Euclidean space. Because we
are using planes and hyperplanes to reduce the amount of computation,
we describe these optimizations as ``geometric''. For comparison, we
briefly discuss optimizations using more traditional optimized dense
linear algebra packages.

\section{Optimization framework}

The tensor paradigm developed in Chapter~\ref{chap:kirby-8} arrives at
the representation
\begin{equation}
  A_{T,i} = \sum_{\alpha\in\mathcal{A}} A^0_{i\alpha} G_T^{\alpha}
  \quad \foralls i \in \mathcal{I},
\end{equation}
or simply
\begin{equation} \label{eq:kirby-4:contract}
  A_T = A^0 : G_T,
\end{equation}
where $\mathcal{I}$ is the set of admissible multi-indices for the element
tensor $A_T$ and $\mathcal{A}$ is the set of admissible multi-indices
for the geometry tensor $G_T$. The reference tensor $ A^0 $ can be
computed at compile-time, and may then be contracted with a $ G_T $ to
obtain the element tensor $A_T$ for each cell $T$ in the finite element
mesh at run-time. The case of computing local finite element stiffness
matrices of size $n_T \times n_T$ corresponds to $\mathcal{I}$ consisting
of $|\mathcal{I}| = n_T^2$ multi-indices of length two, where $n_T$
is the dimension of the local finite element space on~$T$.

It is convenient to recast~\eqref{eq:kirby-4:contract} in terms of a
matrix--vector product:
\begin{equation} \label{eq:kirby-4:tildeag}
  A^0 : G_T \leftrightarrow \tilde{A}^0 \tilde{g}_T.
\end{equation}
Here, the matrix $ \tilde{A} $ lies in $ \mathbb{R}^{|\mathcal{I}| \times
  |\mathcal{A}|} $, and the vector $ \tilde{g}_{T} $ lies in $
\mathbb{R}^{|\mathcal{A}|} $. The resulting matrix--vector product can
then be reshaped into the element tensor $A_T$. As this computation
must occur for each cell $T$ in a finite element mesh, it makes
sense to try to make this operation as efficient as possible.

In the following, we will drop the subscripts and superscripts
of~\eqref{eq:kirby-4:tildeag} and consider the problem of computing a
general matrix--vector product
\begin{equation}
y = A x,
\end{equation}
efficiently, where $ A = \tilde{A}^0 $ is a constant matrix
known \apriori{}, and $x = \tilde{g}_T $is an arbitrary vector. We
will study structure of $ A $ that allows for a reduction in the
number of arithmetic operations required to form these products. With
this structure, we are able to produce a routine that computes the
action of the system in less operations than would be performed using
general sparse or dense linear algebra routines.

Before proceeding with the mathematical formulation, we give an example of
a matrix $ A $ that we would like to optimize. In~\eqref{eq:kirby-4:ALap},
we display the reference tensor $A^0$ for computing a standard stiffness
matrix discretizing a two--dimensional Laplacian with quadratic Lagrange
elements on triangles. The rank four tensor is depicted here as a $6
\times 6$ matrix of $2 \times 2$ matrices.  Full analysis would use a
corresponding flattened $36 \times 4$ matrix $A$.

% Laplacian with quadratic triangular Lagrange elements
\begin{equation} \label{eq:kirby-4:ALap}
A^0 =
\left(
\begin{array}{cc|cc|cc|cc|cc|cc}
3 & 0 & 0 & -1 & 1 & 1 & -4 & -4 & 0 & 4 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\ \hline
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
-1 & 0 & 0 & 3 & 1 & 1 & 0 & 0 & 4 & 0 & -4 & -4 \\ \hline
1 & 0 & 0 & 1 & 3 & 3 & -4 & 0 & 0 & 0 & 0 & -4 \\
1 & 0 & 0 & 1 & 3 & 3 & -4 & 0 & 0 & 0 & 0 & -4 \\ \hline
-4 & 0 & 0 & 0 & -4 & -4 & 8 & 4 & 0 & -4 & 0 & 4 \\
-4 & 0 & 0 & 0 & 0 & 0 & 4 & 8 & -4 & -8 & 4 & 0 \\ \hline
0 & 0 & 0 & 4 & 0 & 0 & 0 & -4 & 8 & 4 & -8 & -4 \\
4 & 0 & 0 & 0 & 0 & 0 & -4 & -8 & 4 & 8 & -4 & 0 \\ \hline
0 & 0 & 0 & -4 & 0 & 0 & 0 & 4 & -8 & -4 & 8 & 4 \\
0 & 0 & 0 & -4 & -4 & -4 & 4 & 0 & -4 & 0 & 4 & 8 \\
\end{array}
\right)
\end{equation}

\section{Topological optimization}

It is possible to apply the matrix $A$, corresponding to the reference
tensor $A^0$ depicted in~\eqref{eq:kirby-4:ALap}, to an arbitrary
vector $x$ in fewer operations than the $144$ multiply--add pairs required
by a standard matrix--vector multiplication. This requires
offline analysis of $A$ and special-purpose code generation that
applies the particular $A$ to a generic $x$. For $ A \in
\mathbb{R}^{M\times N} $, let $\{ a^i \}_{i=1}^M \subset \mathbb{R}^N$
denote the rows of $ A $. The vector $ y = Ax $ may then be computed
by $ M $ dot products of the form $ y_i = a^i x $. Below, we
investigate relationships among the rows of $ A $ to find an optimized
computation of the matrix--vector product.

For the purpose of illustration, we consider the following subset
of~\eqref{eq:kirby-4:ALap}, which would only cost $40$ multiply--add
pairs but contains all the relations we use to optimize the larger version:
\begin{equation} \label{eq:kirby-4:Aflat}
A =
\left(
\begin{array}{c}
  a^1 \leftrightarrow A^0_{1,3} \\
  a^2 \leftrightarrow A^0_{1,4} \\
  a^3 \leftrightarrow A^0_{2,3} \\
  a^4 \leftrightarrow A^0_{3,3} \\
  a^5 \leftrightarrow A^0_{4,6} \\
  a^6 \leftrightarrow A^0_{4,4} \\
  a^7 \leftrightarrow A^0_{4,5} \\
  a^8 \leftrightarrow A^0_{5,6} \\
  a^9 \leftrightarrow A^0_{6,1} \\
  a^{10} \leftrightarrow A^0_{6,6} \\
\end{array}
\right)
 =
\left(
\begin{array}{cccc}
1 & 1 & 0 & 0 \\
-4 & -4 & 0 & 0 \\
0 & 0 & 1 & 1 \\
3 & 3 & 3 & 3 \\
0 & 4 & 4 & 0 \\
8 & 4 & 4 & 8 \\
0 & -4 & -4 & -8 \\
-8 & -4 & -4 & 0 \\
 0 & 0 & 0 & 0 \\
8 & 4 & 4 & 8 \\
\end{array}
\right).
\end{equation}
%
Inspection of~\eqref{eq:kirby-4:Aflat} shows that $ a^9 $ is
zero; therefore, it does not need to be multiplied by the entries of $
x $. In particular, if $ z $ entries of $ a^i $ are zero, then the dot
product $ a^i x $ requires $ N - z $ multiply--add pairs rather than $
N $.

If $ a^i = a^j $ for some $ i \neq j $, as seen in the sixth and tenth
rows of $ A $, then it follows that $ y_i = y_j $, and only one dot
product needs to be performed instead of two. A similar case is where
$\alpha a^i = a^j $ for some number $ \alpha $, as in the first and
second rows of~$A$. This means that after $ y_i $ has been computed,
$y_j = \alpha y_i$ may be computed with a single multiplication.

% I have changed the symbol for Hamming distance here to d_H since d is used later
% as a weight for graph edges.
In addition to equality and collinearity discussed above, one may also
consider other relations between the rows of $A$. Further inspection
of $A$ in~\eqref{eq:kirby-4:Aflat} reveals rows that have some entries
in common but are neither equal nor collinear. Such rows have a small
\emph{Hamming distance}; that is, the number of entries in which the two
rows differ is small. This occurs frequently, as seen in, for example,
rows five and six . We can write $ a^j = a^i + (a^j-a^i) $, where $ a^j
-a^i $ has $ d_H \leqslant N $ nonzero entries and where $d_H$ is the
Hamming distance between $a^i$ and $a^j$. Once $ y_i $ has been computed,
one may thus compute $y_j$ as
\begin{equation}
y_j = y_i + \left( a^j - a^i \right) x,
\end{equation}
which requires only $ d_H $ additional multiply--add pairs. If
$ d_H $ is small compared to $ N $, the savings are considerable.

In \citet{WolfHeath2009}, these binary relations are extended to
include the partial collinearity of two vectors. For example, the
sixth and seventh rows have parts that are collinear, namely
$a^6_{2:4} = - a^7_{2:4}$. This allows $y_j$ to be computed via:
\begin{equation}
  y_j = \alpha(y_i - y_{i,\mathrm{nonmatching}}) + a^j_{\mathrm{nonmatching}} \, x,
\end{equation}
where the subscript indicates non-matching portions of the vectors
padded with zeroes. Such relationships reduce the computation of $y_j$
to the subtraction of the non-matching contributions, a scaling of the
result computed with $y^i$, and then an additional multiplication with
the non-matching entries in $a^j$.

All of these examples of structure relate to either a single row of $
A $ or a pair of rows of $ A $. Such \emph{binary} relations
between pairs of rows are amenable to the formulation of
graph-theoretic structures, as is developed in
Section~\ref{sec:kirby-4:graph}. Higher-order relations also occur
between the rows of $ A $. For example, the first and third rows may
be added and scaled to make the fourth row. In this case, once $ a^1
x $ and $ a^3 x $ are known, the results may be used to compute $
a^{4} x $ using one addition and one multiplication, compared to four
multiplications and three additions for direct evaluation of the dot
product $a^{4} x$.

\section{A graph problem}
\label{sec:kirby-4:graph}

If we restrict consideration to binary relations between the rows of $
A $, we are led naturally to a weighted, undirected graph whose
vertices are the rows $ a^i $ of $A$. An edge between $ a^i $ and $
a^j $ with weight $ d $ indicates that if $ a^i x $ is known, then that
result may be used to compute $ a^j x $ with $ d $ multiply--add pairs. In
practice, such edges also need to be labeled with information indicating
the kind of relationship such as equality, collinearity or a low Hamming distance.

To find the optimal computation through the graph, we use Prim's
algorithm \citep{Prim1957} for computing a minimum-spanning tree. A
minimum spanning tree is a tree that connects all the vertices of the
graph and has minimum total edge
weight. In \citet{KirbyLoggScottEtAl2006}, it is demonstrated that,
under a given set of relationships between rows, a minimum spanning
tree in fact encodes an algorithm that optimally reduces the number of
arithmetic operations required.
This discussion assumes that the initial graph is connected. In
principle, every $ a^i $ is no more than a distance of $ N $ away from
any $ a^j $. In practice, however, only edges with $d < N - z$ are
included in a graph since $N$ is the cost of computing $ y_i $ without
reference to $ y_j $. This often makes the graph unconnected and thus
one must construct a minimum spanning \emph{forest} instead of a tree
(a set of disjoint trees that together touch all the vertices of the
graph). An example of a minimum spanning tree using the binary
relations is shown in Figure~\ref{fig:kirby-4:mst}.

\begin{figure}
  \centering
  \fenicsfig{kirby-4}{small_mst}{\largefig}
  \caption{Minimum spanning tree (forest) for the vectors in
    \eqref{eq:kirby-4:Aflat}. The dashed edges represent edges that
    do not reduce the number of operations (relative to $N - z$) and
    thus disconnect the graph.}
  \label{fig:kirby-4:mst}
\end{figure}

Such a forest may then be used to determine an efficient algorithm for
evaluating $ A x $ as follows. Start with some $ a^i $ and
compute $ y^i = a^i x $ directly in at most $ N $ multiply--add
pairs. The number of multiply--add pairs may be less than $ N $ if one
or more entries of $a^i$ are zero. Then, if $ a^j $ is a nearest
neighbor of $ a^i $ in the forest, use the relationship between $
a^j $ and $ a^i $ to compute $ y^j = a^j x $. After this, take a
nearest neighbor of $ a^j $, and continue until all the entries of
$ y $ have been computed.

Additional improvements may be obtained by recognizing that the input
tensor $ G_T \leftrightarrow x $ is symmetric for certain operators
like the Laplacian. In two spatial dimensions, $ G_T $ for the Laplacian
is $ 2 \times 2 $ with only 3 unique entries, and in three spatial
dimensions it is $ 3 \times 3 $ with only 6 unique entries. This fact
may be used to construct a modified reference tensor $ A^0 $ with
fewer columns. For other operators, it might have symmetry along some
but not all of the axes.

Heath and Wolf proposed a slight variation on this algorithm. Rather
than picking an arbitrary starting row $ a^i $, they enrich the graph
with an extra vertex labeled \textit{IP} for ``inner product.''  Each
$ a^i $ is a distance $ N - z $ from IP, where $ z $ is the number of
vanishing entries in $ a^i $. The IP vertex is always selected as the
root of the minimum spanning tree. It allows for a more robust
treatment of unary relations such as sparsity, and detection of
partial collinearity relations.

\section{Geometric optimization}
\label{sec:kirby-4:geom}

When relations between more than two rows are considered, the
optimization problem may no longer be phrased in terms of a graph, but
requires some other structure. In these cases, proving that one has
found an optimal solution is typically difficult, and it is suspected
that the associated combinatorial problems are $ NP $-hard.

As a first attempt, one can work purely from linear dependencies among
the data as follows. Let $ B=\{ b^i \}_i \subseteq \{ a^i \}_{i=1}^N
$ be a maximal set of nonzero rows of $ A $, such that no two rows are
collinear. Then enumerate all triples which are linearly dependent,
\begin{equation}
  S= \left\{ \left\{ b^i , b^j , b^k \right\} \subseteq B:
  \exists \, \alpha_1, \alpha_2, \alpha_3 \neq 0:
  \alpha_1 b^i + \alpha_2 b^j + \alpha_3 b^k = 0 \right\}.
\end{equation}
The idea is now to identify some subset $C$ of $ B $ that may be used
to recursively construct the rest of the rows in $ B $ using the
relationships in $ S $.

Given some $ C \subset B $, we may define the \emph{closure} of $
C $, denoted by $ \bar{C} $, as follows. First of all, if $ b
\in C $, then $ b \in \bar{C} $. Second, if $ b \in B $ and
there exist $ c,d \in \bar{C} $ such that $ \{b,c,d\} \in S $, then $
b \in \bar{C} $ as well. If $ \bar{C} = B$, we say that $ C $ is
a \emph{generator} for $ B $ or that $ C $
\emph{generates} $ B $.

The recursive definition suggests a greedy process for constructing
the closure of any set $ C $. Each vector in $B$ is put in a priority
queue with an initial value of the cost to compute independent of other
vectors. While $C \ne B$, a vector from $B \backslash C$ with the
minimum cost to compute is added to $C$ and the priorities of $B$ are
updated according to $S$. This process constructs a directed, acyclic
graph that indicates the linear dependence being used. Each $ b \in C
$ will have no out-neighbors, while each $ b \in \bar{C} \backslash C
$ will point to two other members of $ \bar{C} $. This graph is
called a \emph{generating graph}. Using \eqref{eq:kirby-4:Aflat}, we
have the following sets $B$, $S$, and $C$, with the generating graph
shown in Figure~\ref{fig:kirby-4:gg}:
\begin{equation}
\begin{array}{rcl}
 B & = &  \{a_1, a_3, a_4, a_5, a_6, a_7, a_8\} \\
 S & = & \{(a_1, a_3, a_4),(a_4, a_5, a_6), (a_4, a_7, a_8)\} \\
 C & = & \{a_3, a_4, a_5, a_7\} \\
\end{array}
\end{equation}

\begin{figure}
  \centering
  \fenicsfig{kirby-4}{gg2}{\largefig}
  \caption{Generating graph for the vectors in~\eqref{eq:kirby-4:Aflat}.}
  \label{fig:kirby-4:gg}
\end{figure}

If $C$ generates $B$, then the generating graph indicates an optimized
(but perhaps not optimal) process for computing $\{ y^i = b^i x \}_i$.
Take a topological ordering of the vectors $b^i$ according to this
graph. Then, for each $b^i$ in the topological ordering, if $b^i$ has
no out-neighbors, then $b^i x$ is computed explicitly. Otherwise,
$b^i$ will point to two other vectors $b^j$ and $b^k$ for which the
dot products with $x$ will already be known. Since the generating
graph has been built from the set of linearly dependent triples~$S$,
there must exist some $\beta_1, \beta_2$ such that $b^i = \beta_1 b^j
+ \beta_2 b^k$. We may thus compute $y^i$ by
\begin{equation}
y^i = b^i x = \beta_1 b^j x
+ \beta_2  b^k  x,
\end{equation}
which requires only two multiply--add pairs instead of $ N $.

To make best use of the linear dependence information, one would like
to find a generator $C$ that has as few members as possible. We
say that a generator $C$ is \emph{minimal} for $B$ if no $C'
\subset C$ also generates $B$. A stronger requirement is for a
generator to be \emph{minimum}. A generator $C$ is minimum if no other
generator $C'$ has lower cardinality. More complete details and
heuristics for constructing minimal generators are considered
in \citet{KirbyScott2007}; it is not currently known whether such
heuristics construct minimum generators or how hard the problem of
finding minimum generators is.

Given a minimal generator $C$ for $B$, one may consider searching for
higher order linear relations among the elements of $C$, such as sets
of four items that have a three-dimensional span. The discussion of
generating graphs and their utilization is the same in this case.

In \citet{WolfHeath2009}, a combination of the binary and higher-order
relations between the rows of $A$ in a hypergraph model is studied.
While greedy algorithms provide optimal solutions for a graph model,
it is demonstrated that the obvious generalizations to hypergraphs can
be suboptimal. While the hypergraph problems are most likely very
hard, heuristics perform well and provide additional optimizations
beyond the graph models. So, even if a non-optimal solution is found,
it still provides an improved reduction in arithmetic requirements.

\begin{table}
  \centering
    \begin{tabular}{c}
      \emph{triangles} \\
      \begin{tabular}{ccccc}
        \toprule
        degree & $M$ & $N$ & $MN$ & MAPs \\
        \midrule
        1 &6 &3& 18& 9 \\
        2 &21& 3& 63& 17\\
        3 &55& 3& 165& 46 \\
        \bottomrule
      \end{tabular}
      \\
      \\
      \emph{tetrahedra} \\
      \begin{tabular}{ccccc}
        \toprule
        degree & $M$ & $N$ & $MN$ & MAPs \\
        \midrule
        1 & 10 & 6 &60 &27 \\
        2 & 55 & 6 & 330 &101 \\
        3 & 210 &6 &1260 &370 \\
        \bottomrule
      \end{tabular}
    \end{tabular}
    \caption{Number of multiply--add pairs for graph-optimized Laplace
      operator (MAPS) compared to the basic number of multiply--add pairs
      ($MN$).}
    \label{tab:kirby-4:graph}
\end{table}

In Table~\ref{tab:kirby-4:geom}, topological and geometric
optimization are compared for the Laplacian using quadratic through
quartic polynomials on tetrahedra. In the geometric case, the vectors
$a^i$ were filtered for unique direction; that is, only one vector for
each class of collinear vectors was retained. Then, a generating graph
was constructed for the remaining vectors using pairwise linear
dependence. The generator for this set was then searched for linear
dependence among sets of four vectors, and a generating graph
constructed. Perhaps surprisingly, the geometric optimization found
flop reductions comparable to or better than graph-based binary
relations. These are shown in Table~\ref{tab:kirby-4:geom}.

\begin{table}
  \centering
    \begin{tabular}{cccc}
      \toprule
      degree & topological & geometric \\
      \midrule
      2 & 101  & 105   \\
      3 & 370  & 327   \\
      4 & 1118 & 1072  \\
      \bottomrule
    \end{tabular}
    \caption{Comparison of topological and geometric optimizations
      for the Laplace operator on tetrahedra using polynomial degrees two
      through four. In each case, the final number of MAPs for the
      optimized algorithm is reported. The case $q=1$ is not reported
      since then both strategies yield the same number of operations.}
  \label{tab:kirby-4:geom}
\end{table}

\section{Optimization by dense linear algebra}

As an alternative to optimizations that try to find a reduced arithmetic
for computing the element tensor~$A_T$, one may consider computing
the element tensor by efficient dense linear algebra. As above, we
note that the entries of the element tensor $A_T$ may be computed by
the matrix--vector product $\tilde{A}^0 \tilde{g}_T$. Although zeros
may appear in $\tilde{A}^0$, this is typically a dense matrix and so
the matrix--vector product may be computed efficiently with Level~2
BLAS, in particular using a call to~\texttt{dgemv}. There exist a
number of optimized implementations of BLAS, including hand-optimized
vendor implementations, empirically and automatically tuned libraries
\citep{WhaleyPetitetDongarra2001} and formal methods for automatic
derivation of algorithms \citet{BientinesiGunnelsMyersEtAl2005}.

The computation of the element tensor $A_T$ may be optimized further
by recognizing that one may compute the element tensor for a batch of
elements $\{T_i\}_i \subset \mathcal{T}$ in one matrix--matrix
multiplication:
\begin{equation}
  \left[\tilde{A}^0 \tilde{g}_{T_1} \quad \tilde{A}^0 \tilde{g}_{T_2} \quad \cdots \right] =
  \tilde{A}^0 \left[\tilde{g}_{T_1} \quad \tilde{g}_{T_2} \quad \cdots\right].
\end{equation}
This matrix--matrix product may be computed efficiently using a single
Level~3 BLAS call (\texttt{dgemm}) instead of a sequence of Level~2
BLAS calls, and typically leads to better floating-point
performance.

\section{Notes on implementation}

A subset of the optimizations discussed in this chapter are available
as part of the FErari Python module. FErari
(0.2.0) implements optimization based on finding binary relations
between the entries of the element tensor. With optimizations turned
on, FFC calls FErari at compile-time to generate optimized
code. Optimization for FFC can be turned on either by the \texttt{-O}
parameter when FFC is called from the command-line, or by setting
\emp{parameters["form\_compiler"]["optimization"] = True} when FFC is
called as a just-in-time compiler from the DOLFIN Python
interface. Note that the FErari optimizations are only used when FFC
generates code based on the tensor representation described in
Chapter~\ref{chap:kirby-8}. When FFC generates code based on
quadrature, optimization is handled differently, as described in
Chapter~\ref{chap:oelgaard-2}. Improved run-times for several
problems are detailed in \citet{KirbyLogg2008}.
