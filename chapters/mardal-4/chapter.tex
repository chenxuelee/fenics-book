%\documentclass[11pt]{article}
%\usepackage{graphicx}
%\RequirePackage{psfrag}
%\RequirePackage{fancyhdr}
%\RequirePackage{fancybox}
%\RequirePackage{fancyvrb}
%\RequirePackage{sectsty}
%\RequirePackage{amsmath}
%\RequirePackage{amssymb}
%\RequirePackage{makeidx}
%\usepackage{moreverb,relsize,ttname}
%\RequirePackage{url}
%\RequirePackage[latin1]{inputenc}
%\RequirePackage[colorlinks]{hyperref}

%\newcommand{\emp}[1]{{\smaller\texttt{#1}}}
%\newcommand{\bit}{\begin{itemize}}
%\newcommand{\eit}{\end{itemize}}
%\DefineVerbatimEnvironment{code}{Verbatim}{frame=single,rulecolor=\color{blue},fontsize=\small}
\renewcommand{\L}{\mathcal{L}}
\renewcommand{\K}{\mathcal{K}}
\newcommand{\A}{\mathcal{A}}
\newcommand{\B}{\mathcal{B}}
\renewcommand\curl{\operatorname{curl}}

\fenicschapter{Block Preconditioning of Systems of PDEs}
              {Block Preconditioning of Systems of PDEs}
              {Kent-Andre Mardal}
              {block-prec}
%\begin{document}

%\maketitle
%\vspace{-0.3cm}

%\thispagestyle{empty}

\section{Introduction}

In this chapter we will describe the implementation of block preconditioned
Krylov solvers for systems of partial differential equations (PDEs) using
the Python interfaces of Dolfin and Trilinos. We remark that
an alternative to PyTrilinos is PyAMG~\cite{pyamg} which can be used together with PyDolfin. 

An outline of this paper is as follows: First, we review the
abstract theory of constructing preconditioners by considering the
differential operators as mappings in properly chosen Sobolev spaces. 
Second, we will
present several examples, namely the Poisson problem, the Stokes problem, 
the time-dependent Stokes problem and finally a mixed formulation
of the Hodge Laplacian. The code examples related to this chapter can
be found in FENICSBOOK/src/block-prec. 
The code examples presented in this chapter differ slightly from the source
code, in the sense that import statements,  safety checks, command-line arguments, definitions of \emp{Functions}
and \emp{Subdomains} are often removed to shorten the presentation. 

\section{Abstract Framework for Constructing Preconditioners}

This presentation of preconditioning is largely taken from the
review paper~\cite{M-W-09}, where a more comprehensive mathematical
presentation is given. Consider the following abstract formulation
of a linear PDE problem: \\ 
Find $u$ in the Hilbert space $H$ such that: 
\[
\A u = f,  
\]
where $f\in H^*$ and $H^*$ is the dual space of $H$.  
We will assume that the PDE problem is well-posed, i.e., 
that $\A : H \rightarrow H^*$ is a bounded invertible operator in the sense that,  
\[
\|\A\|_{\L (H, H^*)} \le C \quad \mbox{and} \quad  
\|\A^{-1}\|_{\L (H^*, H)} \le C. 
\]
The reader should notice that this operator is bounded only when 
viewed as an operator from $H$ to $H^*$. 
On the other hand, the spectrum of the operator is unbounded. Analogously,  
discretizations of the operator will typically have  
condition numbers that increase 
in powers of $h$, where $h$ is the
characteristic cell size, as the mesh is refined. 
The remedy for the unbounded spectrum is to introduce a preconditioner. 
Let the preconditioner $B$ be an operator
mapping $H^*$ to $H$ such that  
\[
\|\B\|_{\L(H^*, H)} \le C \quad \mbox{and} \quad   
\|\B^{-1}\|_{\L(H, H^*)} \le C. 
\]
Then 
$\B\A: H \rightarrow H$ and 
\[
\|\B\A\|_{\L(H, H)} \le C^2 \quad \mbox{and} \quad  
\|(\B\A)^{-1}\|_{\L(H, H)} \le C^2. 
\]
Hence, the spectrum and therefore the condition number of the
preconditioned operator, 
\[
\kappa(\B\A) = \|\B\A\|_{\L(H, H)} \|(\B\A)^{-1}\|_{\L(H, H)} \le C^4  
\]
is bounded.  
Given that the discretized operators  $\A_h$ and $\B_h$ are stable, in the sense
that the operator norms are bounded, then the condition number of the discrete preconditioned
operator, $\kappa(\B_h \A_h)$, will be bounded independent of $h$. The number of iterations required 
by a Krylov solver to reach a certain convergence criterion can typically be bounded by
the condition number. Hence, when the condition number of the discrete problem is bounded independent
of $h$, the Krylov solvers will have a converges rate independent of $h$. 
If then $\B_h$ is similar to $\A_h$ in terms of storage and evaluation, 
we will then have an \emph{order-optimal} solution algorithm. 
We remark that it is crucial that $\A_h$
is a stable operator and will illustrate this for the Stokes problem. 
Finally we will see that $\B_h$ often can be constructed 
using multigrid techniques. 

\section{Numerical Examples}
\subsection{The Poisson problem with homogeneous Neumann conditions}
The Poisson equation with Neumann conditions reads: \\ 
Find $u$ such that
\begin{eqnarray*}
-\Delta u &= &f \mbox{ in } \Omega,  \\ 
    \frac{\partial u}{\partial n} &=&  g \mbox{ on } \partial \Omega . 
\end{eqnarray*}
The variational problem is: \\ 
Find $u\in H^1_0 \cap L^2_0$ such that 
\[
\int_\Omega \nabla u \cdot \nabla v \, dx = \int_\Omega f v \, dx + \int_{\partial \Omega} g v \, ds, \quad \forall v \in H^1_0 \cap L^2_0 .      
\]
Let the linear operator $\A$ be defined in terms of the  bilinear form,  
\[
(\A u, v) =  \int_\Omega \nabla u \cdot \nabla v \, dx.  
\]
It is well-known that 
$\A$ is a bounded invertible operator from $H^1_0 \cap L^2_0$ into its dual space $H^{-1}\cap L^2_0$. 
Furthermore, it is well-known that one can construct multigrid preconditioners for this operator
such that the preconditioner is spectrally equivalent with the inverse of $A$, independent of the
characteristic size of the cells in the mesh~\cite{JHBramble_1993a,Hackbusch, TOS2001}. 

In this example, we will construct a multigrid preconditioner based on the algebraic multigrid
package ML which is contained in PyTrilinos. Furthermore, we will estimate the eigenvalues
of the preconditioned system. 


First of all, the ML preconditioner is constructed as follows,  
\begin{code}
from PyTrilinos import Epetra, AztecOO, TriUtils, ML 
from dolfin import down_cast, Vector 

class MLPreconditioner: 
    def __init__(self, A): 
        # create the ML preconditioner
        MLList = {
              "max levels"        : 30, 
              "output"            : 1,  
              "smoother: type"    : "ML symmetric Gauss-Seidel",
              "aggregation: type" : "Uncoupled",
              "ML validate parameter list" : False
        }
        ml_prec = ML.MultiLevelPreconditioner(down_cast(A).mat(), 0)
        ml_prec.SetParameterList(MLList)
        ml_prec.ComputePreconditioner()

    def __mul__(self, b):
        # apply the ML preconditioner
        x = Vector(b.size())
        err = self.ml_prec.ApplyInverse(down_cast(b).vec(),
                                        down_cast(x).vec())
        return x
\end{code}
The linear algebra backends uBlas, PETSc and Trilinos all have 
a wide range of Krylov solvers. Here, we implement
these solvers in Python because we would like to store the intermediate
variables and  used them to compute an estimate of the condition number. 
The following code shows the implementation of the Conjugate Gradient method
using the Python linear algebra interface in Dolfin:
\begin{code}
def precondconjgrad_eigest(B, A, x, b, tolerance=1.0E-05, 
                           relativeconv=False, maxiter=500):

    r = b - A*x
    z = B*r
    d = 1.0*z

    rz = inner(r,z)

    if relativeconv: tolerance *= sqrt(rz)
    iter = 0
    alphas = []
    betas = []
    while sqrt(rz) > tolerance and iter <= maxiter:
        z = A*d
        alpha = rz / inner(d,z)
        x += alpha*d
        r -= alpha*z
        z = B*r
        rz_prev = rz
        rz = inner(r,z)
        beta =  rz / rz_prev
        d = z + beta*d
        iter += 1
        alphas.append(alpha)
        betas.append(beta)

    e = eigenvalue_estimates(alphas, betas)
    return x, e, iter
\end{code}
The intermediate variables called alphas and betas can then be used to estimate the condition number of
the preconditioned matrix as follows, see e.g. cite{}. 
Notice that since the preconditioned CG method 
converges quite fast when using AMG as a preconditioner, there will be only a small
number of alphas and betas and we may therefore use the dense linear algebra tools in 
NumPy.
\begin{code}
def eigenvalue_estimates(alphas, betas):
    # eigenvalues estimates in terms of alphas and betas
    import numpy
    from numpy import linalg
    n = len(alphas)
    A = numpy.zeros([n,n])
    A[0,0] = 1/alphas[0]
    for k in range(1, n): 
        A[k,k] = 1/alphas[k] + betas[k-1]/alphas[k-1]
        A[k,k-1] = numpy.sqrt(betas[k-1])/alphas[k-1]
        A[k-1,k] = A[k,k-1]
    e,v = linalg.eig(A) 
    e.sort()

    return x, e
\end{code}
The following code shows the implementation of a Poisson problem solver, using the
above mentioned ML preconditioner and CG algorithm. We remark here that it
is essential for the convergence of the method that both the start vector 
and the right-hand side are both in $L^2_0$. For this reason we subtract the 
mean value from the right hand-side.  The start vector is zero and does therefore
have mean value zero. 
\begin{code}
import Krylov 
import MLPrec 

# use the Epetra backend
parameters["linear_algebra_backend"] = "Epetra"

# Create mesh and finite element
mesh = UnitSquare(10, 10)
V = FunctionSpace(mesh, "CG", 1)

# Define variational problem
v = TestFunction(V)
u = TrialFunction(V)
f = Source(V)
g = Flux(V)
a = dot(grad(v), grad(u))*dx
L = v*f*dx + v*g*ds

# Assemble symmetric matrix and vector
A, b = assemble_system(a,L)

# create solution vector (also used as start vector) 
x = b.copy()
x.zero()

# subtract mean value from right hand-side
c = b.array()
c -= sum(c)/len(c)
b[:] = c  

# create preconditioner
B = MLPrec.MLPreconditioner(A)
x, e, iter = Krylov.precondconjgrad_eigest(B, A, x, b, 10e-6, True, 100)

print "Number of iterations ", iter
print "Eigenvalues ", e 
print "kappa(BA) ", e[len(e)-1]/e[0]
\end{code}
In Table \ref{tabel:neumann} we list the number of iterations for convergence and
the estimated condition number of the preconditioned system based on the code 
shown above, see also the source code \emp{poisson\_neumann.py}. 
\begin{table}
\begin{center}
\begin{tabular}{|c||c|c|c|c|c|c|}
\hline
$h$ & $2^{-4}$ & $2^{-5}$ & $2^{-6}$ & $2^{-7}$ & $2^{-8}$ \\ \hline 
$\kappa$ & 1.56 & 1.26 & 2.09 & 1.49 & 1.20 \\ \hline
$\#iterations$ & 8 & 8 & 9 & 9 & 8 \\ \hline 
\end{tabular}
\caption{The estimated condition number $\kappa$ and the number of iterations for 
  convergence with respect to various mesh refinements.}\label{tabel:neumann} 
\end{center}
\end{table}



\subsection{The Stokes Problem}
Our next example is the Stokes problem, 
\begin{eqnarray}
-\Delta u - \nabla p &=& f \quad \mbox{in} \ \Omega, \\ 
\nabla \cdot u &=& 0 \quad \mbox{in} \  \Omega, \\
             u &=& 0   \quad \mbox{on} \  \partial \Omega.  
\end{eqnarray}
The variational form is: \\ 
Find $u,p \in H^1_0 \times L_0^2$ such that  
\[
\int_\Omega \nabla u : \nabla v \,  dx + 
\int_\Omega \nabla \cdot u \, q \,  dx +  
\int_\Omega \nabla \cdot v \, p \,  dx = \int_\Omega f\, v\, dx   , \quad \forall v,q \in H^1_0 \times L_0^2.
\]
Let the linear operator $\A$ be defined as
\[
\A  =
\begin{pmatrix} A & B^* \\ B & 0 \end{pmatrix}.
\]
where 
\begin{eqnarray}
(A u, v) &=& \int_\Omega \nabla u : \nabla v \,  dx, \\  
(B u, q) &=& \int_\Omega \nabla \cdot u \, q \,  dx,    
\end{eqnarray}
and $B^*$ is the adjoint of $B$.  
Then it is well-known that $\A$ is a bounded operator from
$H^1_0 \times L_0^2$ to its dual $H^{-1} \times L_0^2$, see e.g.~\cite{Brezzi,BrezziFortin}. Therefore, we 
construct a preconditioner, 
$\B: H^{-1} \times L_0^2 \rightarrow H^1_0 \times L_0^2$
defined as 
\[
\B 
= 
\begin{pmatrix} K^{-1} & 0 \\ 0 & L^{-1} \end{pmatrix}.
\]
where 
\begin{eqnarray}
(K u, v) &=& \int_\Omega \nabla u: \nabla v \, dx \\ 
(L p, q) &=& \int_\Omega p \, q \, dx .   
\end{eqnarray}
We refer to \cite{M-W-09} for a mathematical explanation of the derivation 
of such preconditioners. 
Notice that this operator $\B$ is  positive in contrast to $\A$. Hence, 
the preconditioned operator $\B \A$ will be indefinite. For both 
$K$ and $L$, we use the AMG preconditioner provided by ML/Trilinos as
described in the previous example (A simple Jacobi preconditioner
would be sufficient for $L$).
For symmetric  indefinite problems the \emph{Minimum Residual Method} is
the fastest method. Preconditioners of this form has been studied by
many~\cite{E-S-W-text,R-W-92,S-W-93,S-W-94}. In Table \ref{stokes:ex}, 
we present the number of iterations needed for convergence 
and estimates on the condition number $\kappa$ with 
respect to different discretization methods and different characteristic cell sizes $h$. 
The MinRes iteration is stopped when 
$(\B_h r_k, r_k)/(\B_h r_0, r_0) \le 10^{-10}$, where $r_k$ is the residual at
iteration $k$. 
The condition numbers, $\kappa$, were estimated using the CG method on the
normal equation. This condition number will always be less 
than the real condition number and is probably too low
for the last columns for the $P_1-P_1$ method without stabilization. 
for all the. 
Notice that for the stable methods $P_2-P_1$ and $P_2-P_0$,
the number of iterations and the condition number seems to be bounded
independently of $h$. For the unstable $P_1-P_1$ method, the number of iterations
and the condition number increases as $h$ decreases. However, for 
the stabilized $P_1-P_1$ method, where the pressure is stabilized by 
\[
\int_\Omega \nabla \cdot u \, q  - \alpha h^2 \, \nabla p \cdot \nabla q \, dx,       
\]
the number of iterations and the condition number appear to be bounded. 

\begin{table}
\begin{center}
\begin{tabular}{|c|c||c|c|c|c|c|c|}
\hline
method & $h$ & $2^{-4}$ & $2^{-5}$ & $2^{-6}$ & $2^{-7}$ & $2^{-8}$ \\ \hline\hline
$P_2-P_1$ &  iterations & 34 & 43 & 49 & 52 & 58 \\ \hline 
$P_2-P_1$ & $\kappa $ & 13.3 & 13.5 & 13.6 & 13.6 & 13.6 \\ \hline 
$P_2-P_0$ &  iterations & 27 & 35 & 42 & 47 & 54 \\ \hline 
$P_2-P_0$ & $\kappa $ & 6.9 & 7.9 & 8.8 & 9.5 & 10.3 \\ \hline 
$P_1-P_1$ & iterations & 36 & 118 & 200+ & 200+ & 200+ \\ \hline 
$P_1-P_1$ & $\kappa$ & 147 & 308 & 696 & 827 & 671 \\ \hline 
$P_1-P_1$-stab& iterations & 29 & 34 & 33 & 33 & 31 \\ \hline 
$P_1-P_1$-stab& $\kappa $ & 11.0 & 12.3 & 12.5 & 12.6 & 12.7 \\ \hline 
\end{tabular}
\caption{The number of iterations for 
  convergence with respect to mesh refinements.
  The methods $P_2-P_1$, $P_2-P_0$, and $P_1-P_1$-stab are stable, while $P_1-P_1$ is not.}\label{stokes:ex} 
\end{center}
\end{table}

We will now describe the code in detail. 
In this case, the preconditioner consists of two preconditioners.
The following shows how to implement this block preconditioner
based on the ML preconditioner defined in the previous example.
\begin{code}
class SaddlePrec: 
    def __init__(self, K, L):  
        self.K = K
        self.L = L
        self.K_prec = MLPreconditioner(K)
        self.L_prec = MLPreconditioner(L)
        self.n = K.size(0)
        self.m = L.size(0)
        self.x = Vector(self.n+self.m)

    def __mul__(self, b):

        self.x = Vector(self.n+self.m)
        x = self.x 
        n = self.n
        m = self.m

        x[0:n]    = self.K_prec*b[0:n]
        x[n:n+m]  = self.L_prec*b[n:n+m] 

        return x 
\end{code}
The Stokes problem is then specified and solved as follows: 
\begin{code}
mesh = UnitSquare(40,40)
V = VectorFunctionSpace(mesh, "CG", 2)
Q = FunctionSpace(mesh, "CG", 1)
mixed = V + Q 

f = Constant(mesh, (0,0))
g = Constant(mesh, 0)

u, p = TrialFunctions(mixed) 
v, q = TestFunctions(mixed)

a = inner(grad(u), grad(v))*dx + div(u)*q*dx + div(v)*p*dx  
L = dot(f, v)*dx 

bc_func = BoundaryFunction(V)
bc = DirichletBC(V, bc_func, Boundary()) 

A, b = assemble_system(a, L, bc)
\end{code}
And finally, we create a block preconditioner and solve the problem with 
the MinRes method.
\begin{code}
u, p = TrialFunction(V), TrialFunction(Q)
v, q = TestFunction(V), TestFunction(Q)

k = inner(grad(u), grad(v))*dx 
l = p*q*dx 
L0 = dot(v,f)*dx 
L1 = q*g*dx 

K, b0 = assemble_system(k,L0,bc)
L, b1 = assemble_system(l,L1)

x = Vector(b.size())
x.zero()

B = SaddlePrec(K, L)

x, i, rho  = MinRes.precondMinRes(B, A, x, b, 10e-8, False, 200)
\end{code}
We refer to \emp{stokes.py} for the complete code. 

\subsection{The time-dependent Stokes Problem}
Our next example is the time-dependent Stokes problem,
\begin{eqnarray}
u - k \Delta u - \nabla p &=& f \quad \mbox{in} \ \Omega, \\ 
\nabla \cdot u &=& 0 \quad \mbox{in} \  \Omega, \\
             u &=& 0   \quad \mbox{on} \  \partial \Omega.  
\end{eqnarray}
Here $k$ is the time stepping parameter. 

The variational form is: \\ 
Find $u,p \in H^1_0 \times L_0^2$ such that  
\[
\int_\Omega u \cdot v \,  dx + 
k \int_\Omega \nabla u : \nabla v \,  dx + 
\int_\Omega \nabla \cdot u \, q \,  dx +  
\int_\Omega \nabla \cdot v \, p \,  dx = \int_\Omega f\, v\, dx   , \quad \forall v,q \in H^1_0 \times L_0^2.
\]
Let 
\[
\A  =
\begin{pmatrix} A & B^* \\ B & 0 \end{pmatrix}.
\]
where 
\begin{eqnarray}
(A u, v) &=& \int_\Omega u \cdot v \,  dx +  k \int_\Omega \nabla u : \nabla v \,  dx, \\  
(B u, q) &=& \int_\Omega \nabla \cdot u \, q \,  dx,    
\end{eqnarray}
This operator changes character as $k$ varies.  
For $k=1$ the problem behaves like Stokes problem, with 
a non-harmful low order term. However as $k$ approaches
zero the problems change to the mixed formulation of 
a Poisson equation, ie. 
\begin{eqnarray*}
u - \nabla p &=& f, \quad \Omega \\
\nabla \cdot u  &=& 0, \quad \Omega
\end{eqnarray*}
This problem is not an well-defined operator from 
$H^1_0 \times L_0^2$ into its dual. Instead, it
is a mapping from $H(div) \times L_0^2$  to its dual. 
However, as pointed out in \cite{M-W-04} this operator
can also be seen as an operator $L^2 \times H^1$ to its dual.  
In fact, in \cite{M-T-W-02,M-W-04} it was shown that 
 $A$ is a bounded operator from
$L^2 \cap k H^1_0 \times H^1 \cap L_0^2 + k^{-1} L_0^2$ to its dual space
with a bounded inverse. Furthermore, the bounds are uniform in $k$. 
Therefore we 
construct a preconditioner $B$, such that  
\[
\B: L^2 \cap k H^1_0 \times H^1 \cap L_0^2 + k^{-1} L_0^2 \rightarrow 
L^2 + k^{-1} H^{-1} \times H^{-1} \cap L_0^2 + k L_0^2  .
\] 
Such a $\B$ can be defined as 
\[
\B 
= 
\begin{pmatrix} K^{-1} & 0 \\ 0 & L^{-1} + M^{-1} \end{pmatrix}.
\]
where 
\begin{eqnarray}
(K u, v) &=& \int_\Omega \nabla u: \nabla v \, dx \\ 
(L p, q) &=& \int_\Omega k^{-1} p q \, dx \\   
(M p, q) &=& \int_\Omega \nabla p \cdot  \nabla q \, dx .   
\end{eqnarray}
Again we refer to \cite{M-W-09} and references therein,  for an overview of the construction of such preconditioners and 
a more comprehensive mathematical derivation.  
Preconditioners of this form has been studied by many, c.f. e.g. ~\cite{C-C-88, E-S-W-text, M-W-04, M-W-09, T-99}. 

\begin{table}
\begin{center}
\begin{tabular}{|c|c||c|c|c|c|c|c|}
\hline
$k\backslash h$ & $2^{-4}$ & $2^{-5}$ & $2^{-6}$ & $2^{-7}$ & $2^{-8}$ \\ \hline\hline
1.0 & 13.2 & 13.5 & 13.6 & 13.6 & 13.6 \\ \hline 
0.1 & 12.5 & 13.2 & 13.2 & 13.5 & 13.6 \\ \hline 
0.01 & 10.3 & 11.0 & 12.8 & 13.2 & 13.4 \\ \hline 
0.001 & 7.3 & 9.1 & 11.0 & 12.3  & 13.0 \\ \hline 
\end{tabular}
\caption{The  convergence with respect to $k$ and mesh refinements.}\label{timestokes:ex} 
\end{center}
\end{table}

Creating the preconditioner in this example is completely analogous to the Stokes
example except that we need three matrices based on three bilinear forms: 
\begin{code}
k = dot(u, v)*dx +  k*inner(grad(u), grad(v))*dx 
l = kinv*p*q*dx 
m = dot(grad(p),grad(q))*dx 
L0 = dot(v,f)*dx 
L1 = q*g*dx 

K, b0 = assemble_system(k,L0,bc)
L, b1 = assemble_system(l,L1)
M, b1 = assemble_system(m,L1)
\end{code}
Also the code for the block preconditioner is analogous, except that
it is based on three matrices: 
\begin{code}
class SaddlePrec2: 
    def __init__(self, K, L, M):  
        self.K_prec = MLPrec.MLPreconditioner(K)
        self.L_prec = MLPrec.MLPreconditioner(L)
        self.M_prec = MLPrec.MLPreconditioner(M)
        self.n = K.size(0)
        self.m = L.size(0)

    def __mul__(self, b):

        n = self.n
        m = self.m
        x = Vector(n+m)
        y0 = Vector(m)
        y1 = Vector(m)

        x[0:n]    = self.K_prec*b[0:n]
        y0        = self.L_prec*b[n:n+m] 
        y1        = self.M_prec*b[n:n+m] 
        x[n:n+m]   = y0 + y1  

        return x 
\end{code}
The complete code can be found in \emp{timestokes.py}

\subsection{Mixed form of the Hodge Laplacian}
The final example is a mixed formulation
of the Hodge Laplacian, 
\begin{eqnarray}
\nabla \times \nabla \times u - \nabla p &=& f \quad \mbox{in} \ \Omega,    \label{mixed:hodge1} \\
\nabla \cdot u -p &=&  0 \quad \mbox{in} \ \Omega, \label{mixed:hodge2} \\
             u \times n &=&  0 \quad \mbox{on} \ \partial \Omega, \\  
             p          &=&  0 \quad \mbox{on} \ \partial \Omega.  
\end{eqnarray}
The variational form is: \\ 
Find $u, p \in H_0(\curl) \times H^1_0$ such that   
\begin{eqnarray}
\int_\Omega \nabla \times u \cdot \nabla \times v \, dx    
- \int_\Omega \nabla p v \, dx  &=& \int_\Omega f v \, dx  \quad \forall v \in H_0(\curl) \\
 \int_\Omega u  \nabla q  \, dx - \int_\Omega p q \, dx   &=& 0 \quad \forall q \in H^1_0 . \\
\end{eqnarray}
Hence, it is natural to consider preconditioner a for $H(\curl)$ problems
(in addition to $H^1$ preconditioners). Such preconditioners have
been considered by many c.f. e.g. ~\cite{A-F-W-97Hdiv,A-F-W-00, Hip97, Hip99}.   
One important observation in these papers is that point-wise smoothers
are not appropriate for geometric multigrid methods. Furthermore, 
for algebraic multigrid methods, extra care has to be taken for the
aggregation step~\cite{ml-guide,maxwell-amg}. 

Let 
\[
\A  =
\begin{pmatrix} A & B^* \\ B & -C \end{pmatrix}, 
\]
where, 
\begin{eqnarray}
(A u, v) &=& \int_\Omega \nabla \times u \cdot \nabla \times v \, dx, \\    
(B p, v) &=&  - \int_\Omega \nabla p v \, dx \\
(C p, q) &=&  - \int_\Omega p q \, dx 
\end{eqnarray}
Then $\A: H_0(\curl) \times H^1_0 \rightarrow  H^{-1}(\curl) \times H^{-1}$, where   
$H^{-1}(\curl)$ is the dual of   $H_0(\curl)$.


However, if we for the moment forget about the boundary conditions, 
we can obtain the Laplacian form  by eliminating $p$ 
from \eqref{mixed:hodge1}-\eqref{mixed:hodge2}, i.e.,    
\[
\nabla \times \nabla \times u - \nabla \nabla \cdot u = f .  
\]
Hence, the problem is elliptic
in nature, although this is not apparent in the mixed formulation. 
In other words, modulo boundary conditions,  $\A: H^1 \times L^2 \rightarrow  H^{-1} \times L^2$.

To avoid constructing a $H(\curl)$ preconditioner we will employ
the observation that this is a vector Laplacian. 
Let the discrete operator be 
\[
\A  =
\begin{pmatrix} A_h & B_h^* \\ B_h & -C_h \end{pmatrix}, 
\]
where we assume that the discrete system has been obtained by using a stable finite
element method, eg. using lowest order Nedelec elements of first kind~\cite{Nedelec:1980:MFE} combined with continuous piecewise linears.
We eliminate the pressure to obtain the matrix 
\[
K_h  = A_h +  B_h^* C^{-1}_h B_h, 
\]
A problem here is that $C^{-1}_h$ is a dense matrix, therefore we lump the $C_h$ matrix before inverting it, ie.,   
\[
L_h  = A_h +  B_h^* (diag(C_h))^{-1} B_h, 
\]
The matrix $L_h$ is in some sense a vector Laplacian, incorporating the mixed discretization technique, that is appropriate to build an AMG preconditioner upon. 
To test the efficiency of this preconditioner compared with 
more straightforward applications of AMG, we compare a couple of different problems.   
First, we test the preconditioners for the $A_h$ and the $L_h$ operators, 
i.e., we estimate the condition number for the systems 
$P_1 A_h$ and $P_2 L_h$, where $P_1$ and $P_2$ is simply the algebraic multigrid 
preconditioners for $A_h$ and $L_h$, respectively. 
Then we test the preconditioners 
\[
\B_1  =
\begin{pmatrix} A_h & 0  \\ 0  & D_h \end{pmatrix}. 
\]
Here, $D_h$ is a discrete Laplacian. The other preconditioner is   
\[
\B_2  =
\begin{pmatrix} L_h & 0  \\ 0  & C_h \end{pmatrix}. 
\]

In Table \ref{table:hodge} we list the estimated condition numbers on various mesh refinements
on the unitcube. 
\begin{table}
\begin{center}
\begin{tabular}{|c||c|c|c|c|c|c|}
\hline
$h$ & $2^{-1}$ & $2^{-2}$ & $2^{-3}$ & $2^{-4}$  & $2^{-5}$ \\ \hline 
$ P_1 A_h$ & 7.5 & 20.2 & 78.4 & 318.5 & 1206.3 \\ \hline
$ P_2 L_h$ & 1.5 & 2.1  & 5.8  & 20.4 & 78.0 \\ \hline
$\B_1 \A$ & 3.9 & 6.7 & 17.6 & 58.6 & - \\ \hline
$\B_2 \A$ & 4.3 & 8.7 & 33.6 & 30.0 & - \\ \hline
\end{tabular}
\caption{The estimated condition number $\kappa$ and the number of iterations for 
  convergence with respect to various mesh refinements.}  \label{table:hodge}
\end{center}
\end{table}

The main problem in this example is the construction of the preconditioner
for $L_h$. This preconditioner is based on a matrix which is constructed by 
several matrix-matrix products and Dolfin does not provide functionality for
this. However, Epetra/Trilinos have this functionality. To be able 
to use the functionality of Epetra/Trilinos we down\_cast the Dolfin \emp{EpetraMatrix}
to its underlying type \emp{Epetra\_FECrsMatrix}. We may then use PyTrilions. 
The following code demonstrate how to perform matrix matrix products, mass lump inversion  etc. using 
PyTrilinos. 
\begin{code}
u,p = TrialFunction(V), TrialFunction(Q)
v,q = TestFunction(V), TestFunction(Q)

aa = dot(u, v)*dx + dot(curl(v), curl(u))*dx   
bb = dot(grad(p), v)*dx  
ff = q*p*dx  

AA = assemble(aa) 
BB = assemble(bb) 
BF = assemble(bb) 
FF = assemble(ff)

AA_epetra = down_cast(AA).mat()
BB_epetra = down_cast(BB).mat()
BF_epetra = down_cast(BF).mat()
FF_epetra = down_cast(FF).mat()

ff_vec = Epetra.Vector(FF_epetra.RowMap())
FF_epetra.InvRowSums(ff_vec)
BF_epetra.RightScale(ff_vec)

CC = Epetra.FECrsMatrix(Epetra.Copy, AA_epetra.RowMap(), 100)
err = EpetraExt.Multiply(BB_epetra, False, BF_epetra, True, CC) 
DD = EpetraMatrix(CC)

EE = assemble(aa, DD, reset_sparsity=False, add_values=True)
\end{code}
The complete code can be found in \emp{hodge8.py} and \emp{hodge9.py}. 


\section{Conclusion}
In this chapter we have demonstrated that advanced solution algorithms can we developed
relatively easily by using the Python interfaces of Dolfin and Trilinos. The Python linear
algebra interface in Dolfin  allow us to write  Krylov solvers and customize them 
in the language which these algorithms are typically expressed in books. 
Furthermore, it is relatively simple to employ state--of--the--art algebraic multigrid 
algorithms in Python using Trilinos. 

In this chapter we have shown the implementation of block preconditioners for a few selected problems. 
Block preconditioners have been used in a varity of applications, we refer to \cite{M-W-09}
and the references therein for a more complete discussion on this topic. 



