%\documentclass[11pt]{article}
%\usepackage{graphicx}
%\RequirePackage{psfrag}
%\RequirePackage{fancyhdr}
%\RequirePackage{fancybox}
%\RequirePackage{fancyvrb}
%\RequirePackage{sectsty}
%\RequirePackage{amsmath}
%\RequirePackage{amssymb}
%\RequirePackage{makeidx}
%\usepackage{moreverb,relsize,ttname}
%\RequirePackage{url}
%\RequirePackage[latin1]{inputenc}
%\RequirePackage[colorlinks]{hyperref}

%\newcommand{\emp}[1]{{\smaller\texttt{#1}}}
%\newcommand{\bit}{\begin{itemize}}
%\newcommand{\eit}{\end{itemize}}
%\DefineVerbatimEnvironment{code}{Verbatim}{frame=single,rulecolor=\color{blue},fontsize=\small}

\fenicschapter{Block Preconditioning of Systems of PDEs}
              {Block Preconditioning of Systems of PDEs}
              {Kent-Andre Mardal}
              {block-prec}

In this chapter we will describe the implementation of block preconditioned
Krylov solvers for systems of partial differential equations (PDEs) using
the Python interfaces of Dolfin and Trilinos\index{Trilinos}. 
We will start by reviewing the
abstract theory of constructing preconditioners\index{preconditioner} by considering the
differential operators as mappings in properly chosen Sobolev spaces. 
We will then
present several examples, namely the Poisson problem, the Stokes problem, 
the time-dependent Stokes problem and finally a mixed formulation
of the Hodge Laplacian. 

\section{Abstract Framework for Constructing Preconditioners}

This presentation of preconditioning is largely taken from the
review paper~\cite{MardalWinther}, where a more comprehensive mathematical
presentation is given. Consider the following abstract formulation
of a linear PDE problem:  
Find $u$ in a Hilbert space $H$ such that: 
\begin{equation}
\mathcal{A} u = f,  
\end{equation}
where $f\in H'$ and $H'$ is the dual space of $H$.  
We will assume that the PDE problem is well-posed, i.e., 
that $\mathcal{A} : H \rightarrow H'$ is a bounded invertible operator in the sense that,  
\begin{equation}
\|\mathcal{A}\|_{\mathcal{L} (H, H')} \le C \quad \mbox{and} \quad  
\|\mathcal{A}^{-1}\|_{\mathcal{L} (H', H)} \le C. 
\end{equation}
The reader should notice that this operator is bounded only when 
viewed as an operator from $H$ to $H'$. 
The spectrum of the operator is unbounded and   
discretizations of the operator will typically have  
condition numbers that increase 
in negative powers of $h$, where $h$ is the
characteristic cell size, as the mesh is refined. 
The remedy for the unbounded spectrum is to introduce a preconditioner. 
Let the preconditioner $B$ be an operator
mapping $H'$ to $H$ such that  
\begin{equation}
\|\mathcal{B}\|_{\mathcal{L}(H', H)} \le C \quad \mbox{and} \quad   
\|\mathcal{B}^{-1}\|_{\mathcal{L}(H, H')} \le C. 
\end{equation}
Then 
$\mathcal{B}\mathcal{A}: H \rightarrow H$ and 
\begin{equation}
\|\mathcal{B}\mathcal{A}\|_{\mathcal{L}(H, H)} \le C^2 \quad \mbox{and} \quad  
\|(\mathcal{B}\mathcal{A})^{-1}\|_{\mathcal{L}(H, H)} \le C^2. 
\end{equation}
Hence, the spectrum and therefore the condition number of the
preconditioned operator, 
\begin{equation}
\kappa(\mathcal{B}\mathcal{A}) = \|\mathcal{B}\mathcal{A}\|_{\mathcal{L}(H, H)} \|(\mathcal{B}\mathcal{A})^{-1}\|_{\mathcal{L}(H, H)} \le C^4  
\end{equation}
is bounded.  One example of such a preconditioner is the Riesz operator $\mathcal{R}$, i.e.,  the identity mapping between $H'$ and $H$.  
In this case 
\begin{equation}
\|\mathcal{R}\|_{\mathcal{L}(H', H)} = 1 \quad \mbox{and} \quad   
\|\mathcal{R}^{-1}\|_{\mathcal{L}(H, H')} = 1. 
\end{equation}
In fact, in most of our examples the preconditioners are approximate Riesz mappings.  

Given that the discretized operators  $\mathcal{A}_h$ and $\mathcal{B}_h$ are stable, in the sense
that the operator norms are bounded, ie.,  
 \begin{equation}
\|\mathcal{A}_h\|_{\mathcal{L} (H, H')} \le C, \ \|\mathcal{A}_h^{-1}\|_{\mathcal{L} (H', H)} \le C, \ 
\|\mathcal{B}_h\|_{\mathcal{L}(H', H)} \le C,  \  \|\mathcal{B}_h^{-1}\|_{\mathcal{L}(H, H')} \le C, 
\end{equation}
then the condition number of the discrete preconditioned
operator, $\kappa(\mathcal{B}_h \mathcal{A}_h)$, will be bounded by $C^4$ independently of $h$. 
Furthermore, the number of iterations required 
by a Krylov solver to reach a certain convergence criterion can typically be bounded by
the condition number. Hence, when the condition number of the discrete problem is bounded independent
of $h$, the Krylov solvers will have a convergence rate that is independent of $h$. 
If $\mathcal{B}_h$ is similar to $\mathcal{A}_h$ in terms of storage and evaluation, 
then the solution algorithm is \emph{order-optimal}. 
We remark that it is crucial that $\mathcal{A}_h$
is a stable operator and we will illustrate what happens for unstable operators in the example concerning Stokes problem. 
Finally, we will see that $\mathcal{B}_h$ often can be constructed using multigrid techniques. These multigrid
preconditioners will be spectrally equivalent with the Riesz mappings. 

\section{Numerical Examples}
In all examples the mesh will be refinements of the unit square or unit cube. 
The code examples presented in this chapter differ slightly from the source
code, in the sense that import statements,  safety checks, command-line arguments, definitions of \emp{Functions}
and \emp{Subdomains} are often removed to shorten the presentation. 


\subsection{The Poisson problem with homogeneous Neumann conditions}
The Poisson equation with Neumann conditions reads: \\ 
Find $u$ such that
\begin{eqnarray}
-\Delta u &= &f \mbox{ in } \Omega,  \\ 
    \frac{\partial u}{\partial n} &=&  g \mbox{ on } \partial \Omega . 
\end{eqnarray}
The corresponding variational problem is:  Find $u\in H^1 \cap L^2_0$ such that 
\[
\int_\Omega \nabla u \cdot \nabla v \, dx = \int_\Omega f v \, dx + \int_{\partial \Omega} g v \, ds, \quad\forall v \in H^1 \cap L^2_0 .      
\]
Let the linear operator $\mathcal{A}$ be defined in terms of the  bilinear form,  
\[
(\mathcal{A} u, v) =  \int_\Omega \nabla u \cdot \nabla v \, dx.  
\]
It is well-known that 
$\mathcal{A}$ is a bounded invertible operator from $H^1 \cap L^2_0$ into its dual space. 
Furthermore, it is well-known that one can construct multigrid preconditioners for this operator
such that the preconditioner is spectrally equivalent with the inverse of $\mathcal{A}$, independent of the
characteristic size of the cells in the mesh~\cite{Bramble1993,Hackbusch, TOS2001}. 

In this example, we use a multigrid preconditioner based on the algebraic multigrid
package ML contained in PyTrilinos\index{PyTrilinos}. Furthermore, we will estimate the eigenvalues
of the preconditioned system. We use continuous piecewise linear elements and
compute the condition number of the preconditioned system and corresponding 
number of iteration required for convergence using the Conjugate Gradient method\index{Conjugate Gradient method} for various refinements of the unit square. 


First of all, the ML preconditioner is constructed as follows,  
\begin{python}
from PyTrilinos import Epetra, AztecOO, TriUtils, ML 
from dolfin import down_cast, Vector 

class MLPreconditioner: 
    def __init__(self, A): 
        # create the ML preconditioner
        MLList = {
              "max levels"        : 30, 
              "output"            : 1,  
              "smoother: type"    : "ML symmetric Gauss-Seidel",
              "aggregation: type" : "Uncoupled",
              "ML validate parameter list" : False
        }
        ml_prec = ML.MultiLevelPreconditioner(down_cast(A).mat(), 0)
        ml_prec.SetParameterList(MLList)
        ml_prec.ComputePreconditioner()

    def __mul__(self, b):
        # apply the ML preconditioner
        x = Vector(b.size())
        err = self.ml_prec.ApplyInverse(down_cast(b).vec(),
                                        down_cast(x).vec())
        return x
\end{python}
The linear algebra backends uBlas, PETSc and Trilinos all have 
a wide range of Krylov solvers. Here, we implement
these solvers in Python because we need to store intermediate
variables and  used them to compute an estimate of the condition number. 
The following code shows the implementation of the Conjugate Gradient method
using the Python linear algebra interface in Dolfin:
\begin{python}
def precondconjgrad_eigest(B, A, x, b, tolerance=1.0E-05, 
                           relativeconv=False, maxiter=500):

    r = b - A*x
    z = B*r
    d = 1.0*z

    rz = inner(r,z)

    if relativeconv: tolerance *= sqrt(rz)
    iter = 0
    alphas = []
    betas = []
    while sqrt(rz) > tolerance and iter <= maxiter:
        z = A*d
        alpha = rz / inner(d,z)
        x += alpha*d
        r -= alpha*z
        z = B*r
        rz_prev = rz
        rz = inner(r,z)
        beta =  rz / rz_prev
        d = z + beta*d
        iter += 1
        alphas.append(alpha)
        betas.append(beta)

    e = eigenvalue_estimates(alphas, betas)
    return x, e, iter
\end{python}
The intermediate variables called alphas and betas can then be used to estimate the condition number of
the preconditioned matrix as follows, see eg.~\cite{Saad2003}. 
Notice that since the preconditioned Conjugate Gradient method 
converges quite fast when using algebraic multigrid (AMG)\index{AMG} as a preconditioner, there will be only a small
number of alphas and betas. Therefore we use the dense linear algebra tools in 
NumPy to compute the eigenvalue estimates. 
\begin{python}
def eigenvalue_estimates(alphas, betas):
    # eigenvalues estimates in terms of alphas and betas
    import numpy
    from numpy import linalg
    n = len(alphas)
    A = numpy.zeros([n,n])
    A[0,0] = 1/alphas[0]
    for k in range(1, n): 
        A[k,k] = 1/alphas[k] + betas[k-1]/alphas[k-1]
        A[k,k-1] = numpy.sqrt(betas[k-1])/alphas[k-1]
        A[k-1,k] = A[k,k-1]
    e,v = linalg.eig(A) 
    e.sort()

    return x, e
\end{python}
The following code shows the implementation of a Poisson problem solver, using the
above mentioned ML preconditioner and Conjungate Gradient algorithm. We remark here that it
is essential for the convergence of the method that both the start vector 
and the right-hand side are both in $L^2_0$. For this reason we subtract the 
mean value from the right hand-side.  The start vector is zero and does therefore
have mean value zero. 
\begin{python}
import Krylov 
import MLPrec 

# use the Epetra backend
parameters["linear_algebra_backend"] = "Epetra"

# Create mesh and finite element
mesh = UnitSquare(10, 10)
V = FunctionSpace(mesh, "CG", 1)

# Define variational problem
v = TestFunction(V)
u = TrialFunction(V)
f = Source(V)
g = Flux(V)
a = dot(grad(v), grad(u))*dx
L = v*f*dx + v*g*ds

# Assemble symmetric matrix and vector
A, b = assemble_system(a,L)

# create solution vector (also used as start vector) 
x = b.copy()
x.zero()

# subtract mean value from right hand-side
c = b.array()
c -= sum(c)/len(c)
b[:] = c  

# create preconditioner
B = MLPrec.MLPreconditioner(A)
x, e, iter = Krylov.precondconjgrad_eigest(B, A, x, b, 10e-6, True, 100)

print "Number of iterations ", iter
print "Eigenvalues ", e 
print "kappa(BA) ", e[len(e)-1]/e[0]
\end{python}
In Table~\ref{tabel:neumann} we list the number of iterations for convergence and
the estimated condition number of the preconditioned system based on the code 
shown above. We use test different refinements of the unit square and 
continuous piecewise linear elements, $\mathrm{CG}_1$. The source function is $f= 500\exp(-((x-0.5)^2 + (y-0.5)^2)/0.02)$ and
the boundary condition is $g =  25 \sin(5\pi y)$ for $x=0$ and zero elsewhere, see also the source code \emp{poisson\_neumann.py}. 
\begin{table}
\begin{center}
\begin{tabular}{|c||c|c|c|c|c|c|}
\hline
$h$ & $2^{-4}$ & $2^{-5}$ & $2^{-6}$ & $2^{-7}$ & $2^{-8}$ \\ \hline 
$\kappa$ & 1.56 & 1.26 & 2.09 & 1.49 & 1.20 \\ \hline
$\#iterations$ & 8 & 8 & 9 & 9 & 8 \\ \hline 
\end{tabular}
\caption{The estimated condition number $\kappa$ and the number of iterations for 
  convergence with respect to various mesh refinements for the Poisson 
  problem with Neumann conditions and
  the $\mathrm{CG}_1$ method.}\label{tabel:neumann} 
\end{center}
\end{table}



\subsection{The Stokes Problem}
Our next example is the Stokes problem, 
\begin{eqnarray}
-\Delta u - \nabla p &=& f \quad \mbox{in} \ \Omega, \\ 
\nabla \cdot u &=& 0 \quad \mbox{in} \  \Omega, \\
             u &=& g   \quad \mbox{on} \  \partial \Omega.  
\end{eqnarray}
The variational form is: \\ 
Find $u,p \in H^1_g \times L_0^2$ such that  
\[
\int_\Omega \nabla u : \nabla v \,  dx + 
\int_\Omega \nabla \cdot u \, q \,  dx +  
\int_\Omega \nabla \cdot v \, p \,  dx = \int_\Omega f\, v\, dx   , \quad \forall v,q \in H^1_0 \times L_0^2.
\]
Let the linear operator $\mathcal{A}$ be defined as
\[
\mathcal{A}  =
\begin{pmatrix} A & B^* \\ B & 0 \end{pmatrix}.
\]
where 
\begin{eqnarray}
(A u, v) &=& \int_\Omega \nabla u : \nabla v \,  dx, \\  
(B u, q) &=& \int_\Omega \nabla \cdot u \, q \,  dx,    
\end{eqnarray}
and $B^*$ is the adjoint of $B$.  
Then it is well-known that $\mathcal{A}$ is a bounded operator from
$H^1_g \times L_0^2$ to its dual $H_g^{-1} \times L_0^2$, see e.g.~\cite{Brezzi1974,BrezziFortin}. Therefore, we 
construct a preconditioner, 
$\mathcal{B}: H_g^{-1} \times L_0^2 \rightarrow H^1_g \times L_0^2$
defined as 
\[
\mathcal{B} 
= 
\begin{pmatrix} K^{-1} & 0 \\ 0 & L^{-1} \end{pmatrix}.
\]
where 
\begin{eqnarray}
(K u, v) &=& \int_\Omega \nabla u: \nabla v \, dx \\ 
(L p, q) &=& \int_\Omega p \, q \, dx .   
\end{eqnarray}
We refer to \cite{MardalWinther} for a mathematical explanation of the derivation 
of such preconditioners. 
Notice that this operator $\mathcal{B}$ is  positive in contrast to $\mathcal{A}$. Hence, 
the preconditioned operator $\mathcal{B} \mathcal{A}$ will be indefinite. For both 
$K$ and $L$, we use the AMG preconditioner provided by ML/Trilinos as
described in the previous example (A simple Jacobi preconditioner
would be sufficient for $L$).
For symmetric  indefinite problems the \emph{Minimum Residual Method} is
the fastest method. Preconditioners of this form has been studied by
many~\cite{ElmanSilvesterWathen2005,R-W-92,S-W-93,S-W-94}. 

In Table~\ref{stokes:ex} 
we present the number of iterations needed for convergence 
and estimates on the condition number $\kappa$ with 
respect to different discretization methods and different characteristic cell sizes $h$. The problem we
are solving is the so-called lid driven cavity problem, ie., $f=0$ and $g = (1,0)$ for $y=1$ and zero elsewhere.   
We use different mixed methods, namely  the  $\mathrm{CG}_2-\mathrm{CG}_1$, $\mathrm{CG}_2-\mathrm{DG}_0$,  
$\mathrm{CG}_1-\mathrm{CG}_1$, and  $\mathrm{CG}_1-\mathrm{CG}_1$ stabilized.  
The MinRes iteration is stopped when 
$(\mathcal{B}_h r_k, r_k)/(\mathcal{B}_h r_0, r_0) \le 10^{-10}$, where $r_k$ is the residual at
iteration $k$. 
The condition numbers, $\kappa$, were estimated using the Conjugate Gradient method on the
normal equation. This condition number will always be less 
than the real condition number and is probably too low
for the last columns for the $\mathrm{CG}_1-\mathrm{CG}_1$ method without stabilization. 
for all the. 
Notice that for the stable methods $\mathrm{CG}_2-\mathrm{CG}_1$ and $\mathrm{CG}_2-\mathrm{DG}_0$,
the number of iterations and the condition number seems to be bounded
independently of $h$. For the unstable $\mathrm{CG}_1-\mathrm{CG}_1$ method, the number of iterations
and the condition number increases as $h$ decreases. However, for 
the stabilized method, $\mathrm{CG}_1-\mathrm{CG}_1$-stab, where the pressure is stabilized by 
\[
\int_\Omega \nabla \cdot u \, q  - \alpha h^2 \, \nabla p \cdot \nabla q \, dx,       
\]
with $\alpha=0.01$, the number of iterations and the condition number appear to be bounded. 

\begin{table}
\begin{center}
\begin{tabular}{|c|c||c|c|c|c|c|c|}
\hline
method & $h$ & $2^{-4}$ & $2^{-5}$ & $2^{-6}$ & $2^{-7}$ & $2^{-8}$ \\ \hline\hline
$\mathrm{CG}_2-\mathrm{CG}_1$ &  iterations & 34 & 43 & 49 & 52 & 58 \\ \hline 
$\mathrm{CG}_2-\mathrm{CG}_1$ & $\kappa $ & 13.3 & 13.5 & 13.6 & 13.6 & 13.6 \\ \hline 
$\mathrm{CG}_2-\mathrm{DG}_0$ &  iterations & 27 & 35 & 42 & 47 & 54 \\ \hline 
$\mathrm{CG}_2-\mathrm{DG}_0$ & $\kappa $ & 6.9 & 7.9 & 8.8 & 9.5 & 10.3 \\ \hline 
$\mathrm{CG}_1-\mathrm{CG}_1$ & iterations & 36 & 118 & 200+ & 200+ & 200+ \\ \hline 
$\mathrm{CG}_1-\mathrm{CG}_1$ & $\kappa$ & 147 & 308 & 696 & 827 & 671 \\ \hline 
$\mathrm{CG}_1-\mathrm{CG}_1$-stab& iterations & 29 & 34 & 33 & 33 & 31 \\ \hline 
$\mathrm{CG}_1-\mathrm{CG}_1$-stab& $\kappa $ & 11.0 & 12.3 & 12.5 & 12.6 & 12.7 \\ \hline 
\end{tabular}
\caption{The number of iterations for 
  convergence with respect to mesh refinements.
  The methods $\mathrm{CG}_2-\mathrm{CG}_1$, $\mathrm{CG}_2-\mathrm{DG}_0$, and $\mathrm{CG}_1-\mathrm{CG}_1$-stab are stable, while $\mathrm{CG}_1-\mathrm{CG}_1$ is not.}\label{stokes:ex} 
\end{center}
\end{table}

We will now describe the code in detail. 
In this case, the preconditioner consists of two preconditioners.
The following shows how to implement this block preconditioner\index{block preconditioner}
based on the ML preconditioner defined in the previous example.
\begin{python}
class SaddlePrec: 
    def __init__(self, K, L):  
        self.K = K
        self.L = L
        self.K_prec = MLPreconditioner(K)
        self.L_prec = MLPreconditioner(L)
        self.n = K.size(0)
        self.m = L.size(0)
        self.x = Vector(self.n+self.m)

    def __mul__(self, b):

        self.x = Vector(self.n+self.m)
        x = self.x 
        n = self.n
        m = self.m

        x[0:n]    = self.K_prec*b[0:n]
        x[n:n+m]  = self.L_prec*b[n:n+m] 

        return x 
\end{python}
The Stokes problem is then specified and solved as follows: 
\begin{python}
mesh = UnitSquare(40,40)
V = VectorFunctionSpace(mesh, "CG", 2)
Q = FunctionSpace(mesh, "CG", 1)
mixed = V + Q 

f = Constant(mesh, (0,0))
g = Constant(mesh, 0)

u, p = TrialFunctions(mixed) 
v, q = TestFunctions(mixed)

a = inner(grad(u), grad(v))*dx + div(u)*q*dx + div(v)*p*dx  
L = dot(f, v)*dx 

bc_func = BoundaryFunction(V)
bc = DirichletBC(V, bc_func, Boundary()) 

A, b = assemble_system(a, L, bc)
\end{python}
And finally, we create a block preconditioner and solve the problem with 
the MinRes method\index{Minimal Residual Method}.
\begin{python}
u, p = TrialFunction(V), TrialFunction(Q)
v, q = TestFunction(V), TestFunction(Q)

k = inner(grad(u), grad(v))*dx 
l = p*q*dx 
L0 = dot(v,f)*dx 
L1 = q*g*dx 

K, b0 = assemble_system(k,L0,bc)
L, b1 = assemble_system(l,L1)

x = Vector(b.size())
x.zero()

B = SaddlePrec(K, L)

x, i, rho  = MinRes.precondMinRes(B, A, x, b, 10e-8, False, 200)
\end{python}
We refer to \emp{stokes.py} for the complete code. 

\subsection{The time-dependent Stokes Problem}
Our next example is the time-dependent Stokes problem,
\begin{eqnarray}
u - k \Delta u - \nabla p &=& f \quad \mbox{in} \ \Omega, \\ 
\nabla \cdot u &=& 0 \quad \mbox{in} \  \Omega, \\
             u &=& 0   \quad \mbox{on} \  \partial \Omega.  
\end{eqnarray}
Here $k$ is the time stepping parameter. 

The variational form is: \\ 
Find $u,p \in H^1_0 \times L_0^2$ such that  
\[
\int_\Omega u \cdot v \,  dx + 
k \int_\Omega \nabla u : \nabla v \,  dx + 
\int_\Omega \nabla \cdot u \, q \,  dx +  
\int_\Omega \nabla \cdot v \, p \,  dx = \int_\Omega f\, v\, dx   , \quad \forall v,q \in H^1_0 \times L_0^2.
\]
Let 
\[
\mathcal{A}  =
\begin{pmatrix} A & B^* \\ B & 0 \end{pmatrix}.
\]
where 
\begin{eqnarray}
(A u, v) &=& \int_\Omega u \cdot v \,  dx +  k \int_\Omega \nabla u : \nabla v \,  dx, \\  
(B u, q) &=& \int_\Omega \nabla \cdot u \, q \,  dx,    
\end{eqnarray}
This operator changes character as $k$ varies.  
For $k=1$ the problem behaves like Stokes problem, with 
a non-harmful low order term. However as $k$ approaches
zero the problems change to the mixed formulation of 
a Poisson equation, ie. 
\begin{eqnarray}
u - \nabla p &=& f, \quad \Omega \\
\nabla \cdot u  &=& 0, \quad \Omega
\end{eqnarray}
This problem is not an well-defined operator from 
$H^1_0 \times L_0^2$ into its dual. Instead, it
is a mapping from $H(div) \times L_0^2$  to its dual. 
However, as pointed out in \cite{MardalWinther2004} this operator
can also be seen as an operator $L^2 \times H^1$ to its dual.  
In fact, in \cite{MardalTaiWinther2002a,M-W-04} it was shown that 
 $A$ is a bounded operator from
$L^2 \cap k^{1/2} H^1_0 \times H^1 \cap L_0^2 + k^{-1/2} L_0^2$ to its dual space
with a bounded inverse. Furthermore, the bounds are uniform in $k$. 
Therefore we 
construct a preconditioner $B$, such that  
\[
\mathcal{B}: L^2 \cap k^{1/2} H^1_0 \times H^1 \cap L_0^2 + k^{-1/2} L_0^2 \rightarrow 
L^2 + k^{-1/2} H^{-1} \times H^{-1} \cap L_0^2 + k^{1/2} L_0^2  .
\] 
Such a $\mathcal{B}$ can be defined as 
\[
\mathcal{B} 
= 
\begin{pmatrix} K^{-1} & 0 \\ 0 & L^{-1} + M^{-1} \end{pmatrix}.
\]
where 
\begin{eqnarray}
(K u, v) &=& \int_\Omega \nabla u: \nabla v \, dx \\ 
(L p, q) &=& \int_\Omega k^{-1} p q \, dx \\   
(M p, q) &=& \int_\Omega \nabla p \cdot  \nabla q \, dx .   
\end{eqnarray}
Again we refer to \cite{MardalWinther} and references therein,  for an overview and more comprehensive mathematical derivation 
of the construction of such preconditioners.  
Preconditioners of this form has been studied by many, c.f. e.g. ~\cite{CahouetChabard1988, E-S-W-text, M-W-04, M-W-09, T-99}. 

\begin{table}
\begin{center}
\begin{tabular}{|c|c||c|c|c|c|c|c|}
\hline
$k\backslash h$ & $2^{-4}$ & $2^{-5}$ & $2^{-6}$ & $2^{-7}$ & $2^{-8}$ \\ \hline\hline
1.0 & 13.2 & 13.5 & 13.6 & 13.6 & 13.6 \\ \hline 
0.1 & 12.5 & 13.2 & 13.2 & 13.5 & 13.6 \\ \hline 
0.01 & 10.3 & 11.0 & 12.8 & 13.2 & 13.4 \\ \hline 
0.001 & 7.3 & 9.1 & 11.0 & 12.3  & 13.0 \\ \hline 
\end{tabular}
\caption{The convergence with respect to $k$ and mesh refinements for the time-dependent Stokes problem 
when using the $\mathrm{CG}_2-\mathrm{CG}_1$ method.}\label{timestokes:ex} 
\end{center}
\end{table}

Creating the preconditioner in this example is completely analogous to the Stokes
example except that we need three matrices based on three bilinear forms: 
\begin{python}
k = dot(u, v)*dx +  k*inner(grad(u), grad(v))*dx 
l = kinv*p*q*dx 
m = dot(grad(p),grad(q))*dx 
L0 = dot(v,f)*dx 
L1 = q*g*dx 

K, b0 = assemble_system(k,L0,bc)
L, b1 = assemble_system(l,L1)
M, b1 = assemble_system(m,L1)
\end{python}
Also the code for the block preconditioner is analogous, except that
it is based on three matrices: 
\begin{python}
class SaddlePrec2: 
    def __init__(self, K, L, M):  
        self.K_prec = MLPrec.MLPreconditioner(K)
        self.L_prec = MLPrec.MLPreconditioner(L)
        self.M_prec = MLPrec.MLPreconditioner(M)
        self.n = K.size(0)
        self.m = L.size(0)

    def __mul__(self, b):

        n = self.n
        m = self.m
        x = Vector(n+m)
        y0 = Vector(m)
        y1 = Vector(m)

        x[0:n]    = self.K_prec*b[0:n]
        y0        = self.L_prec*b[n:n+m] 
        y1        = self.M_prec*b[n:n+m] 
        x[n:n+m]   = y0 + y1  

        return x 
\end{python}
In Table~\ref{timestokes:ex} we show the condition number for the time-dependent Stokes problem descretized
with the $\mathrm{CG}_2-\mathrm{CG}_1$ method for various mesh refinements and values of $k$. We have the 
same boundary conditions as for the Stokes problem, ie., $f=0$ and $g = (1,0)$ for $y=1$ and zero elsewhere. 
Clearly, the condition number appears to be bounded by $\approx 14$,  although the asymptotic limit 
is not reached for small $k$ on these coarse meshes. 
The complete code can be found in \emp{timestokes.py}

\subsection{Mixed form of the Hodge Laplacian}
The final example is a mixed formulation
of the Hodge Laplacian, 
\begin{eqnarray}
\nabla \times \nabla \times u - \nabla p &=& f \quad \mbox{in} \ \Omega,    \label{mixed:hodge1} \\
\nabla \cdot u -p &=&  0 \quad \mbox{in} \ \Omega, \label{mixed:hodge2} \\
             u \times n &=&  0 \quad \mbox{on} \ \partial \Omega, \\  
             p          &=&  0 \quad \mbox{on} \ \partial \Omega.  
\end{eqnarray}
The variational form is: \\ 
Find $u, p \in H_0(\operatorname{curl}) \times H^1_0$ such that   
\begin{eqnarray}
\int_\Omega \nabla \times u \cdot \nabla \times v \, dx    
- \int_\Omega \nabla p v \, dx  &=& \int_\Omega f v \, dx  \quad \forall v \in H_0(\operatorname{curl}) \\
 \int_\Omega u  \nabla q  \, dx - \int_\Omega p q \, dx   &=& 0 \quad \forall q \in H^1_0 . 
\end{eqnarray}
Hence, it is natural to consider preconditioner a for $H(\operatorname{curl})$ problems
(in addition to $H^1$ preconditioners). Such preconditioners have
been considered by many c.f. e.g. ~\cite{ArnoldFalkWinther1997Hdiv,A-F-W-00, Hip97, Hip99}.   
One important observation in these papers is that point-wise smoothers
are not appropriate for geometric multigrid methods. Furthermore, 
for algebraic multigrid methods, extra care has to be taken for the
aggregation step~\cite{GeeSiefertHuEtAl2006,maxwell-amg}. 

Let 
\[
\mathcal{A}  =
\begin{pmatrix} A & B^* \\ B & -C \end{pmatrix}, 
\]
where, 
\begin{eqnarray}
(A u, v) &=& \int_\Omega \nabla \times u \cdot \nabla \times v \, dx, \\    
(B p, v) &=&  - \int_\Omega \nabla p v \, dx \\
(C p, q) &=&  - \int_\Omega p q \, dx 
\end{eqnarray}
Then $\mathcal{A}: H_0(\operatorname{curl}) \times H^1_0 \rightarrow  H^{-1}(\operatorname{curl}) \times H^{-1}$, where   
$H^{-1}(\operatorname{curl})$ is the dual of   $H_0(\operatorname{curl})$.


However, if we for the moment forget about the boundary conditions, 
we can obtain the Laplacian form  by eliminating $p$ 
from \eqref{mixed:hodge1}-\eqref{mixed:hodge2}, i.e.,    
\[
\nabla \times \nabla \times u - \nabla \nabla \cdot u = f .  
\]
Hence, the problem is elliptic
in nature and modulo boundary conditions,  $\mathcal{A}: H^1 \times L^2 \rightarrow  H^{-1} \times L^2$.

To avoid constructing a $H(\operatorname{curl})$ preconditioner we will employ
the observation that this is a vector Laplacian. 
Let the discrete operator be 
\[
\mathcal{A}  =
\begin{pmatrix} A_h & B_h^* \\ B_h & -C_h \end{pmatrix}, 
\]
where we assume that the discrete system has been obtained by using a stable finite
element method. We eliminate the pressure to obtain the matrix 
\[
K_h  = A_h +  B_h^* C^{-1}_h B_h, 
\]
A problem here is that $C^{-1}_h$ is a dense matrix. However, we lump $C_h$ matrix before inverting it, ie.,   
\[
L_h  = A_h +  B_h^* (diag(C_h))^{-1} B_h, 
\]
The matrix $L_h$ is then in some sense a vector Laplacian, incorporating the mixed discretization technique. 
To test the efficiency of this preconditioner compared with 
more straightforward applications of AMG, we compare a couple of different problems.   
First, we test the preconditioners for the $A_h$ and the $L_h$ operators, 
i.e., we estimate the condition number for the systems 
$P_1 A_h$ and $P_2 L_h$, where $P_1$ and $P_2$ is simply the algebraic multigrid 
preconditioners for $A_h$ and $L_h$, respectively. 
Then we test the preconditioners 
\[
\mathcal{B}_1  =
\begin{pmatrix} A_h & 0  \\ 0  & D_h \end{pmatrix}. 
\]
Here, $D_h$ is a discrete Laplacian. The other preconditioner is   
\[
\mathcal{B}_2  =
\begin{pmatrix} L_h & 0  \\ 0  & C_h \end{pmatrix}. 
\]


The main implementational problem in this example is the construction of the preconditioner
for $L_h$. This preconditioner is based on a matrix which is constructed by 
several matrix-matrix products and Dolfin does not provide functionality for
this. However, Epetra/Trilinos have this functionality. To be able 
to use the functionality of Epetra/Trilinos we down\_cast the Dolfin \emp{EpetraMatrix}
to its underlying type \emp{Epetra\_FECrsMatrix}. We may then use PyTrilions. 
The following code demonstrate how to perform matrix matrix products, mass lump inversion  etc. using 
PyTrilinos. 
\begin{python}
V = FunctionSpace(mesh, "N1curl", 1)
Q = FunctionSpace(mesh, "CG", 1)

u,p = TrialFunction(V), TrialFunction(Q)
v,q = TestFunction(V), TestFunction(Q)

aa = dot(u, v)*dx + dot(curl(v), curl(u))*dx   
bb = dot(grad(p), v)*dx  
ff = q*p*dx  

AA = assemble(aa) 
BB = assemble(bb) 
BF = assemble(bb) 
FF = assemble(ff)

AA_epetra = down_cast(AA).mat()
BB_epetra = down_cast(BB).mat()
BF_epetra = down_cast(BF).mat()
FF_epetra = down_cast(FF).mat()

ff_vec = Epetra.Vector(FF_epetra.RowMap())
FF_epetra.InvRowSums(ff_vec)
BF_epetra.RightScale(ff_vec)

CC = Epetra.FECrsMatrix(Epetra.Copy, AA_epetra.RowMap(), 100)
err = EpetraExt.Multiply(BB_epetra, False, BF_epetra, True, CC) 
DD = EpetraMatrix(CC)

EE = assemble(aa, DD, reset_sparsity=False, add_values=True)
\end{python}
The complete code can be found in \emp{hodge8.py} and \emp{hodge9.py}. 

In Table~\ref{table:hodge} we list the estimated condition numbers on various mesh refinements
on the unitcube. We use the  lowest order Nedelec elements of first kind~\cite{Nedelec1980a} combined with continuous piecewise linears.
In this example we use homogeneous boundary conditions and $f=0$, but we use a random start vector. 
\begin{table}
\begin{center}
\begin{tabular}{|c||c|c|c|c|c|c|}
\hline
$h$ & $2^{-1}$ & $2^{-2}$ & $2^{-3}$ & $2^{-4}$  & $2^{-5}$ \\ \hline 
$ P_1 A_h$ & 7.5 & 20.2 & 78.4 & 318.5 & 1206.3 \\ \hline
$ P_2 L_h$ & 1.5 & 2.1  & 5.8  & 20.4 & 78.0 \\ \hline
$\mathcal{B}_1 \mathcal{A}$ & 3.9 & 6.7 & 17.6 & 58.6 & - \\ \hline
$\mathcal{B}_2 \mathcal{A}$ & 4.3 & 8.7 & 33.6 & 30.0 & - \\ \hline
\end{tabular}
\caption{The estimated condition number $\kappa$ and the number of iterations for 
  convergence with respect to various mesh refinements and preconditioners for the mixed formulation of the Hodge Laplacian.}  \label{table:hodge}
\end{center}
\end{table}



\section{Conclusion}
In this chapter we have demonstrated that advanced solution algorithms can we developed
relatively easily by using the Python interfaces of Dolfin and Trilinos. The Python linear
algebra interface in Dolfin  allow us to write  Krylov solvers and customize them 
in the language which these algorithms are typically expressed in books. 
Furthermore, it is relatively simple to employ state--of--the--art algebraic multigrid 
algorithms in Python using Trilinos. 
We remark that
an alternative to PyTrilinos is PyAMG~\cite{BellOlsonSchroder2009} which can be used together with PyDolfin. 


We have shown the implementation of block preconditioners for a few selected problems. 
Block preconditioners have been used in a varity of applications, we refer to \cite{MardalWinther}
and the references therein for a more complete discussion on this topic. 
For an overview of similar and alternative preconditioning techniques cf. eg.~\cite{BenziGolubLiesen2005,E-S-W-text}. 



